services:
  # Vector Database
  qdrant:
    image: qdrant/qdrant:latest
    container_name: rag_qdrant
    ports:
      - "6333:6333"
      - "6334:6334"
    volumes:
      - qdrant_storage:/qdrant/storage
    environment:
      - QDRANT__SERVICE__HTTP_PORT=6333
      - QDRANT__SERVICE__GRPC_PORT=6334

  # Redis for background jobs
  redis:
    image: redis:7-alpine
    container_name: rag_redis
    ports:
      - "6379:6379"
    volumes:
      - redis_data:/data
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 30s
      timeout: 10s
      retries: 3

  # FastAPI Backend
  api:
    build:
      context: ./services/api
      dockerfile: Dockerfile
    container_name: rag_api
    ports:
      - "8000:8000"
    environment:
      - QDRANT_URL=http://qdrant:6333
      - REDIS_URL=redis://redis:6379/0
      - OPENAI_API_KEY=${OPENAI_API_KEY:-}
      - EMBEDDING_MODEL=${EMBEDDING_MODEL:-text-embedding-3-small}
      - LLM_MODEL=${LLM_MODEL:-gpt-4o-mini}
      - USE_LOCAL_LLM=true
      - OLLAMA_BASE_URL=http://ollama:11434
      - LOCAL_LLM_MODEL=${LOCAL_LLM_MODEL:-llama3.1:8b}
      - USE_LOCAL_EMBEDDINGS=true
      - LOCAL_EMBEDDING_MODEL=sentence-transformers/all-MiniLM-L12-v2
      - UPLOAD_DIR=/app/uploads
      - MAX_UPLOAD_SIZE_MB=50
      - CHUNK_SIZE=1000
      - CHUNK_OVERLAP=200
      - EMBEDDING_VERSION=v1
    volumes:
      - uploads_data:/app/uploads
    depends_on:
      - qdrant
      - redis
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/v1/health"]
      interval: 30s
      timeout: 10s
      retries: 3

  # Background Worker
  worker:
    build:
      context: ./services/api
      dockerfile: Dockerfile
    container_name: rag_worker
    command: ["python", "-m", "worker"]
    environment:
      - QDRANT_URL=http://qdrant:6333
      - REDIS_URL=redis://redis:6379/0
      - OPENAI_API_KEY=${OPENAI_API_KEY:-}
      - EMBEDDING_MODEL=${EMBEDDING_MODEL:-text-embedding-3-small}
      - LLM_MODEL=${LLM_MODEL:-gpt-4o-mini}
      - USE_LOCAL_LLM=true
      - OLLAMA_BASE_URL=http://ollama:11434
      - LOCAL_LLM_MODEL=${LOCAL_LLM_MODEL:-llama3.1:8b}
      - USE_LOCAL_EMBEDDINGS=true
      - LOCAL_EMBEDDING_MODEL=sentence-transformers/all-MiniLM-L12-v2
      - UPLOAD_DIR=/app/uploads
      - MAX_UPLOAD_SIZE_MB=50
      - CHUNK_SIZE=1000
      - CHUNK_OVERLAP=200
      - EMBEDDING_VERSION=v1
    volumes:
      - uploads_data:/app/uploads
    depends_on:
      - api
      - redis
    restart: unless-stopped

  # Streamlit Frontend
  ui:
    build:
      context: ./services/ui
      dockerfile: Dockerfile
    container_name: rag_ui
    ports:
      - "8501:8501"
    environment:
      - API_BASE_URL=http://api:8000
    depends_on:
      - api
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8501/_stcore/health"]
      interval: 30s
      timeout: 10s
      retries: 3

  # Ollama for local LLM
  ollama:
    image: ollama/ollama:latest
    container_name: rag_ollama
    ports:
      - "11434:11434"
    volumes:
      - ollama_data:/root/.ollama
    environment:
      - OLLAMA_HOST=0.0.0.0
    restart: unless-stopped

volumes:
  qdrant_storage:
    driver: local
  redis_data:
    driver: local
  uploads_data:
    driver: local
  ollama_data:
    driver: local

networks:
  default:
    name: rag_network
